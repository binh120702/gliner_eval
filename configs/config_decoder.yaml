# Model Configuration
model_name: microsoft/deberta-v3-small # Hugging Face model
labels_encoder: null
labels_decoder: HuggingFaceTB/SmolLM2-135M-Instruct #"openai-community/gpt2"
name: "span level gliner"
max_width: 12
hidden_size: 512
dropout: 0.3
fine_tune: true
subtoken_pooling: first
fuse_layers: false
post_fusion_schema: null
decoder_mode: "span"
full_decoder_context: true
span_mode: markerV1

# Training Parameters
num_steps: 300000
train_batch_size: 4
eval_every: 300
warmup_ratio: 0.05
scheduler_type: "cosine"

# loss function
loss_alpha: 0.75
loss_gamma: 0
label_smoothing: 0
loss_reduction: "mean"

# Learning Rate and weight decay Configuration
lr_encoder: 2e-5
lr_others: 3e-5
weight_decay_encoder: 0.01
weight_decay_other: 0.01

max_grad_norm: 10.0

# Directory Paths
root_dir: gliner_logs
train_data: "data/nuner_train.json" # see https://github.com/urchade/GLiNER/tree/main/data
val_data_dir: "none"
# "NER_datasets": val data from the paper can be obtained from "https://drive.google.com/file/d/1T-5IbocGka35I7X3CE6yKe5N_Xg2lVKT/view"

# Pretrained Model Path
prev_path: null

save_total_limit: 3 #maximum amount of checkpoints to save

# Advanced Training Settings
size_sup: -1
max_types: 30
shuffle_types: true
random_drop: true
max_neg_type_ratio: 1
max_len: 512
freeze_token_rep: false
